{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Texas Power Demand - AutoML Tabular Forecasting Model for Batch Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This sample demonstrates how to use the Vertex SDK to create a tabular forecasting model using a BigQuery Omni dataset, and then performa batch predictions using a Google Cloud [AutoML](https://cloud.google.com/vertex-ai/docs/start/automl-users) forecasting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"fsi-select-demo\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://fsi-select-demo-ml-artifacts/forecasting_v1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2022-02-23T17:04:45Z  gs://fsi-select-demo-ml-artifacts/forecasting_v1/#1645635885626488  metageneration=1\n",
      "TOTAL: 1 objects, 0 bytes (0 B)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-cloud-aiplatform               1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl"
   },
   "source": [
    "# Tutorial\n",
    "\n",
    "Now you are ready to start creating your own AutoML tabular forecasting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now set the variable `TRAINING_DATASET_BQ_PATH` to the location of the BigQuery table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "import_file:covid,csv,forecast"
   },
   "outputs": [],
   "source": [
    "pwr_demand_TRAINING_DATASET_BQ_PATH = (\n",
    "    \"bq://fsi-select-demo:mysql_dataset_log1.ds-train-test_texas_power_demand\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,forecast"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TimeSeriesDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `bq_source`: Alternatively, import data items from a BigQuery table into the `Dataset` resource.\n",
    "\n",
    "This operation may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "create_dataset:tabular,forecast"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TimeSeriesDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TimeSeriesDataset backing LRO: projects/355426521391/locations/us-central1/datasets/4357289814085599232/operations/4792626668416008192\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TimeSeriesDataset created. Resource name: projects/355426521391/locations/us-central1/datasets/4357289814085599232\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TimeSeriesDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TimeSeriesDataset('projects/355426521391/locations/us-central1/datasets/4357289814085599232')\n",
      "projects/355426521391/locations/us-central1/datasets/4357289814085599232\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.TimeSeriesDataset.create(\n",
    "    display_name=\"train_power_demand_texas\" + \"_\" + TIMESTAMP,\n",
    "    bq_source=[pwr_demand_TRAINING_DATASET_BQ_PATH],\n",
    ")\n",
    "\n",
    "time_column = \"date\"\n",
    "time_series_identifier_column = \"state\"\n",
    "target_column = \"demand_mgwhr\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "set_transformations:covid"
   },
   "outputs": [],
   "source": [
    "COLUMN_SPECS = {\n",
    "    time_column: \"timestamp\",\n",
    "    target_column: \"numeric\",\n",
    "    \"ds\": \"timestamp\",\n",
    "    \"hr\": \"numeric\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:tabular,forecast"
   },
   "source": [
    "### Create and run training job\n",
    "\n",
    "To train an AutoML model, you perform two steps: 1) create a training job, and 2) run the job.\n",
    "\n",
    "#### Create training job\n",
    "\n",
    "An AutoML training job is created with the `AutoMLForecastingTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
    "- `column_transformations`: (Optional): Transformations to apply to the input columns\n",
    "- `optimization_objective`: The optimization objective to minimize or maximize.\n",
    "    - `minimize-rmse`\n",
    "    - `minimize-mae`\n",
    "    - `minimize-rmsle`\n",
    "\n",
    "The instantiated object is the job for the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "create_automl_pipeline:tabular,forecast"
   },
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = f\"texas-power-demand-forecast-model_{TIMESTAMP}\"\n",
    "\n",
    "training_job = aiplatform.AutoMLForecastingTrainingJob(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    optimization_objective=\"minimize-rmse\",\n",
    "    column_specs=COLUMN_SPECS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:forecast"
   },
   "source": [
    "#### Run the training pipeline\n",
    "\n",
    "Next, you start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `dataset`: The `Dataset` resource to train the model.\n",
    "- `model_display_name`: The human readable name for the trained model.\n",
    "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
    "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
    "- `target_column`: The name of the column to train as the label.\n",
    "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
    "- `time_column`:\n",
    "- `time_series_identifier_column`:\n",
    "\n",
    "The `run` method when completed returns the `Model` resource.\n",
    "\n",
    "The execution of the training pipeline will take up to one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "run_automl_pipeline:forecast"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:No dataset split provided. The service will use a default split.\n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6916877017245810688?project=355426521391\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLForecastingTrainingJob run completed. Resource name: projects/355426521391/locations/us-central1/trainingPipelines/6916877017245810688\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/355426521391/locations/us-central1/models/7573334937051332608\n"
     ]
    }
   ],
   "source": [
    "model = training_job.run(\n",
    "    dataset=dataset,\n",
    "    target_column=target_column,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    available_at_forecast_columns=[time_column],\n",
    "    unavailable_at_forecast_columns=[target_column],\n",
    "    time_series_attribute_columns=[\"ds\", \"hr\"],\n",
    "    forecast_horizon=24,\n",
    "    context_window=24,\n",
    "    data_granularity_unit=\"hour\",\n",
    "    data_granularity_count=1,\n",
    "    weight_column=None,\n",
    "    budget_milli_node_hours=1000,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    predefined_split_column_name=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "source": [
    "## Review model evaluation scores\n",
    "After your model has finished training, you can review the evaluation scores for it.\n",
    "\n",
    "First, you need to get a reference to the new model. As with datasets, you can either use the reference to the model variable you created when you deployed the model or you can list all of the models in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/355426521391/locations/us-central1/models/7573334937051332608/evaluations/4729850347024198656\"\n",
      "metrics_schema_uri: \"gs://google-cloud-aiplatform/schema/modelevaluation/forecasting_metrics_1.0.0.yaml\"\n",
      "metrics {\n",
      "  struct_value {\n",
      "    fields {\n",
      "      key: \"meanAbsoluteError\"\n",
      "      value {\n",
      "        number_value: 4283.304\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"meanAbsolutePercentageError\"\n",
      "      value {\n",
      "        number_value: 9.718367\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"rSquared\"\n",
      "      value {\n",
      "        number_value: 0.64353895\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"rootMeanSquaredError\"\n",
      "      value {\n",
      "        number_value: 5707.3203\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"rootMeanSquaredLogError\"\n",
      "      value {\n",
      "        number_value: 0.13297832\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "create_time {\n",
      "  seconds: 1646624219\n",
      "  nanos: 285172000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model resource ID\n",
    "models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "model = models[0]\n",
    "\n",
    "# Get a reference to the Model Service client\n",
    "client_options = aiplatform.initializer.global_config.get_client_options()\n",
    "model_service_client = aiplatform.gapic.ModelServiceClient(\n",
    "    client_options=client_options\n",
    ")\n",
    "\n",
    "model_evaluations = model_service_client.list_model_evaluations(\n",
    "    parent=model.resource_name\n",
    ")\n",
    "model_evaluation = list(model_evaluations)[0]\n",
    "print(model_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Send a batch prediction request\n",
    "\n",
    "Send a batch prediction to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "source": [
    "### Make the batch prediction request\n",
    "\n",
    "Now that your Model resource is trained, you can make a batch prediction by invoking the batch_predict() method, with the following parameters:\n",
    "\n",
    "- `job_display_name`: The human readable name for the batch prediction job.\n",
    "- `gcs_source`: A list of one or more batch request input files.\n",
    "- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.\n",
    "- `instances_format`: The format for the input instances, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
    "- `predictions_format`: The format for the output predictions, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
    "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsi-select-demo\n"
     ]
    }
   ],
   "source": [
    "print(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2c9d935c6ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bigquery dataset fsi-select-demo.texas_power_demand_predictions_20220307020547 in us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# bq_dataset_name_prefix = \"vertex_forecasting_predictions\"\n",
    "\n",
    "batch_predict_bq_output_dataset_name = f\"texas_power_demand_predictions_{TIMESTAMP}\"\n",
    "batch_predict_bq_output_dataset_path = \"{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")\n",
    "\n",
    "batch_predict_bq_output_uri_prefix = \"bq://{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")\n",
    "\n",
    "# Must be the same region as batch_predict_bq_input_uri\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "bq_dataset = bigquery.Dataset(batch_predict_bq_output_dataset_path)\n",
    "dataset_region = \"us-central1\"  # @param {type : \"string\"}\n",
    "bq_dataset.location = dataset_region\n",
    "bq_dataset = client.create_dataset(bq_dataset)\n",
    "print(\n",
    "    \"Created bigquery dataset {} in {}\".format(\n",
    "        batch_predict_bq_output_dataset_path, dataset_region\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_bq_output_uri_prefix = \"bq://{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "<google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7fc6a1df8c50> is waiting for upstream dependencies to complete.\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/3550999245739786240?project=355426521391\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_DATASET_BQ_PATH = (\n",
    "    f\"bq://fsi-select-demo:mysql_dataset_log1.ds-validation_texas_power_demand\"\n",
    ")\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=f\"texas_power_demand_predictions_{TIMESTAMP}\",\n",
    "    bigquery_source=PREDICTION_DATASET_BQ_PATH,\n",
    "    instances_format=\"bigquery\",\n",
    "    bigquery_destination_prefix=batch_predict_bq_output_uri_prefix,\n",
    "    predictions_format=\"bigquery\",\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_prediction_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### Wait for completion of batch prediction job\n",
    "\n",
    "Next, wait for the batch job to complete. Alternatively, you can set the parameter `sync` to `True` in the `batch_predict()` method to block until the batch prediction job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/355426521391/locations/us-central1/batchPredictionJobs/3550999245739786240\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,forecast"
   },
   "source": [
    "### Get the predictions\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a CSV format:\n",
    "\n",
    "- CSV header + predicted_label\n",
    "- CSV row + prediction, per prediction request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,forecast"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_prediction_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78080aa9088e"
   },
   "source": [
    "### Visualize the forecasts\n",
    "\n",
    "Lastly, follow the given link to visualize the generated forecasts in [Data Studio](https://support.google.com/datastudio/answer/6283323?hl=en).\n",
    "The code block included in this section dynamically generates a Data Studio link that specifies the template, the location of the forecasts, and the query to generate the chart. The data is populated from the forecasts generated earlier.\n",
    "\n",
    "You can inspect the used template at https://datastudio.google.com/c/u/0/reporting/067f70d2-8cd6-4a4c-a099-292acd1053e8. This was created by Google specifically to view forecasting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "f82f00be2160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://datastudio.google.com/c/u/0/reporting?params=%7B%22templateId%22%3A%22067f70d2-8cd6-4a4c-a099-292acd1053e8%22%2C%22ds0.connector%22%3A%22BIG_QUERY%22%2C%22ds0.projectId%22%3A%22fsi-select-demo%22%2C%22ds0.billingProjectId%22%3A%22fsi-select-demo%22%2C%22ds0.type%22%3A%22CUSTOM_QUERY%22%2C%22ds0.sql%22%3A%22SELECT+%5Cn+CAST%28input.date+as+DATETIME%29+timestamp_col%2C%5Cn+CAST%28input.state+as+STRING%29+time_series_identifier_col%2C%5Cn+CAST%28input.demand_mgwhr+as+NUMERIC%29+historical_values%2C%5Cn+CAST%28predicted_demand_mgwhr.value+as+NUMERIC%29+predicted_values%2C%5Cn+%2A+%5CnFROM+%60fsi-select-demo.mysql_dataset_log1.ds-validation_texas_power_demand%60+input%5CnLEFT+JOIN+%60fsi-select-demo.texas_power_demand_predictions_20220307020547.predictions_2022_03_06T20_54_29_921Z%60+output%5CnON%5CnCAST%28input.date+as+DATETIME%29+%3D+CAST%28output.date+as+DATETIME%29%5CnAND+CAST%28input.state+as+STRING%29+%3D+CAST%28output.state+as+STRING%29%22%7D\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "tables = client.list_tables(batch_predict_bq_output_dataset_path)\n",
    "\n",
    "prediction_table_id = \"\"\n",
    "for table in tables:\n",
    "    if (\n",
    "        table.table_id.startswith(\"predictions_\")\n",
    "        and table.table_id > prediction_table_id\n",
    "    ):\n",
    "        prediction_table_id = table.table_id\n",
    "batch_predict_bq_output_uri = \"{}.{}\".format(\n",
    "    batch_predict_bq_output_dataset_path, prediction_table_id\n",
    ")\n",
    "\n",
    "\n",
    "def _sanitize_bq_uri(bq_uri):\n",
    "    if bq_uri.startswith(\"bq://\"):\n",
    "        bq_uri = bq_uri[5:]\n",
    "    return bq_uri.replace(\":\", \".\")\n",
    "\n",
    "\n",
    "def get_data_studio_link(\n",
    "    batch_prediction_bq_input_uri,\n",
    "    batch_prediction_bq_output_uri,\n",
    "    time_column,\n",
    "    time_series_identifier_column,\n",
    "    target_column,\n",
    "):\n",
    "    batch_prediction_bq_input_uri = _sanitize_bq_uri(batch_prediction_bq_input_uri)\n",
    "    batch_prediction_bq_output_uri = _sanitize_bq_uri(batch_prediction_bq_output_uri)\n",
    "    base_url = \"https://datastudio.google.com/c/u/0/reporting\"\n",
    "    query = (\n",
    "        \"SELECT \\\\n\"\n",
    "        \" CAST(input.{} as DATETIME) timestamp_col,\\\\n\"\n",
    "        \" CAST(input.{} as STRING) time_series_identifier_col,\\\\n\"\n",
    "        \" CAST(input.{} as NUMERIC) historical_values,\\\\n\"\n",
    "        \" CAST(predicted_{}.value as NUMERIC) predicted_values,\\\\n\"\n",
    "        \" * \\\\n\"\n",
    "        \"FROM `{}` input\\\\n\"\n",
    "        \"LEFT JOIN `{}` output\\\\n\"\n",
    "        \"ON\\\\n\"\n",
    "        \"CAST(input.{} as DATETIME) = CAST(output.{} as DATETIME)\\\\n\"\n",
    "        \"AND CAST(input.{} as STRING) = CAST(output.{} as STRING)\"\n",
    "    )\n",
    "    query = query.format(\n",
    "        time_column,\n",
    "        time_series_identifier_column,\n",
    "        target_column,\n",
    "        target_column,\n",
    "        batch_prediction_bq_input_uri,\n",
    "        batch_prediction_bq_output_uri,\n",
    "        time_column,\n",
    "        time_column,\n",
    "        time_series_identifier_column,\n",
    "        time_series_identifier_column,\n",
    "    )\n",
    "    params = {\n",
    "        \"templateId\": \"067f70d2-8cd6-4a4c-a099-292acd1053e8\",\n",
    "        \"ds0.connector\": \"BIG_QUERY\",\n",
    "        \"ds0.projectId\": PROJECT_ID,\n",
    "        \"ds0.billingProjectId\": PROJECT_ID,\n",
    "        \"ds0.type\": \"CUSTOM_QUERY\",\n",
    "        \"ds0.sql\": query,\n",
    "    }\n",
    "    params_str_parts = []\n",
    "    for k, v in params.items():\n",
    "        params_str_parts.append('\"{}\":\"{}\"'.format(k, v))\n",
    "    params_str = \"\".join([\"{\", \",\".join(params_str_parts), \"}\"])\n",
    "    return \"{}?{}\".format(base_url, urllib.parse.urlencode({\"params\": params_str}))\n",
    "\n",
    "\n",
    "print(\n",
    "    get_data_studio_link(\n",
    "        PREDICTION_DATASET_BQ_PATH,\n",
    "        batch_predict_bq_output_uri,\n",
    "        time_column,\n",
    "        time_series_identifier_column,\n",
    "        target_column,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources:\n",
    "\n",
    "- Dataset\n",
    "- AutoML Training Job\n",
    "- Model\n",
    "- Batch Prediction Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Delete dataset\n",
    "dataset.delete()\n",
    "\n",
    "# Training job\n",
    "training_job.delete()\n",
    "\n",
    "# Delete model\n",
    "model.delete()\n",
    "\n",
    "# Delete batch prediction job\n",
    "batch_prediction_job.delete()\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_NAME\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk_automl_tabular_forecasting_batch.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
